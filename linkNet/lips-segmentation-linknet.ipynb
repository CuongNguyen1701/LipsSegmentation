{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8273422,"sourceType":"datasetVersion","datasetId":4910824},{"sourceId":58816,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":49246}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch.nn import LazyLinear\nfrom PIL import Image\nimport requests\nfrom torch.optim import Adam\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimage_size = (256,256)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-31T09:02:56.450231Z","iopub.execute_input":"2024-05-31T09:02:56.450649Z","iopub.status.idle":"2024-05-31T09:03:00.074030Z","shell.execute_reply.started":"2024-05-31T09:02:56.450604Z","shell.execute_reply":"2024-05-31T09:03:00.073184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, kernel_size = 3):\n        super(BasicBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = kernel_size, stride = stride, padding = (kernel_size - 1)//2)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout(p=0.1)\n            \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.dropout(out) \n        return out\n    \nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels, strided = True):\n        super(EncoderBlock, self).__init__()\n        out_channels = in_channels*2 if strided else in_channels\n        self.layer1 = BasicBlock(in_channels, out_channels, stride=2 if strided else 1)\n        self.layer2 = BasicBlock(out_channels, out_channels)\n        self.layer3 = BasicBlock(out_channels, out_channels)\n        self.layer4 = BasicBlock(out_channels, out_channels)\n        self.downsample = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2 if strided else 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        residual1 = self.downsample(x)\n        out = self.layer2(out) + residual1\n        residual2 = out\n        out = self.layer3(out)\n        out = self.layer4(out) + residual2\n        return out\n    \nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(DecoderBlock, self).__init__()\n        self.layer1 = BasicBlock(in_channels, in_channels // 4, kernel_size = 1)\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels // 4, in_channels // 4, kernel_size = 3, stride = 2, padding=1, output_padding=1),\n            nn.BatchNorm2d(in_channels // 4)\n        )\n        self.layer3 = BasicBlock(in_channels // 4, in_channels // 2, kernel_size = 1)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        return out\n    \nclass InitialBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(InitialBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n    \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.maxpool(out)\n        return out\n    \nclass FinalBlock(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(FinalBlock, self).__init__()\n        \n        self.transposeconv1 = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size = 3, stride = 2, output_padding=0)\n        self.bn1 = nn.BatchNorm2d(in_channels // 2)\n        \n        self.conv1 = nn.Conv2d(in_channels // 2, in_channels // 2, kernel_size = 2)\n        self.bn2 = nn.BatchNorm2d(in_channels // 2)\n        \n        self.conv2 = nn.Conv2d(in_channels // 2, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(num_classes)\n        \n        self.sigmoid = nn.Sigmoid()      \n        \n    def forward(self, x):\n        out = self.transposeconv1(x)\n        out = self.bn1(out)\n        out = self.conv1(out)\n        out = self.bn2(out)\n        out = self.conv2(out)\n        out = self.bn3(out)\n        out = self.sigmoid(out)\n        return out\n    \nclass LinkNet(nn.Module):\n    def __init__(self, num_classes):\n        super(LinkNet, self).__init__()\n        self.initblock = InitialBlock(3, 64)\n        self.encoder1 = EncoderBlock(64, strided = False)\n        self.encoder2 = EncoderBlock(64)\n        self.encoder3 = EncoderBlock(128)\n#         self.encoder4 = EncoderBlock(256)\n#         self.decoder4 = DecoderBlock(512)\n        self.decoder3 = DecoderBlock(256)\n        self.decoder2 = DecoderBlock(128)\n        self.decoder1 = DecoderBlock(64)\n        self.finalblock = FinalBlock(32, num_classes)\n        \n    def forward(self, x):\n        out = self.initblock(x)\n        residual1 = self.encoder1(out)\n        residual2 = self.encoder2(residual1)\n        out = self.encoder3(residual2)\n#         residual3 = self.encoder3(residual2)\n#         out = self.encoder4(residual3)\n#         out = self.decoder4(out) + residual3\n        out = self.decoder3(out) + residual2\n        out = self.decoder2(out) + residual1\n        out = self.decoder1(out)\n        out = self.finalblock(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.075702Z","iopub.execute_input":"2024-05-31T09:03:00.076305Z","iopub.status.idle":"2024-05-31T09:03:00.102256Z","shell.execute_reply.started":"2024-05-31T09:03:00.076276Z","shell.execute_reply":"2024-05-31T09:03:00.101290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom copy import deepcopy\nimport matplotlib.pyplot as plt\n\nnum_classes = 1\n\nmodel = LinkNet(num_classes)\nmodel_path = '/kaggle/input/linknet_lip_seg/pytorch/best/1/latest_lips_segmentation_linknet.pt'\nstate_dict = torch.load(model_path)\nnew_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\nmodel.load_state_dict(new_state_dict)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Assuming that we are on a machine with multiple GPUs.\nif torch.cuda.device_count() > 1:\n    print(torch.cuda.device_count(), \"GPUs!\")\n    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n    model = nn.DataParallel(model)\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.103595Z","iopub.execute_input":"2024-05-31T09:03:00.103928Z","iopub.status.idle":"2024-05-31T09:03:00.528385Z","shell.execute_reply.started":"2024-05-31T09:03:00.103893Z","shell.execute_reply":"2024-05-31T09:03:00.527388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom torch.utils.data import Dataset\nimport os\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, transform_image=None, transform_mask=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_names = sorted(os.listdir(img_dir))\n        self.mask_names = sorted(os.listdir(mask_dir))\n        self.transform_image = transform_image\n        self.transform_mask = transform_mask\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_names[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_names[idx])\n        image = Image.open(img_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')\n\n        if self.transform_image and self.transform_mask:\n            image = self.transform_image(image)\n            mask = self.transform_mask(mask)\n            mask = mask.squeeze(0)\n\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.531238Z","iopub.execute_input":"2024-05-31T09:03:00.531793Z","iopub.status.idle":"2024-05-31T09:03:00.540660Z","shell.execute_reply.started":"2024-05-31T09:03:00.531761Z","shell.execute_reply":"2024-05-31T09:03:00.539604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import functional as F_tr\nfrom torchvision import transforms\nfrom torchvision.transforms import v2\n\nclass ToGrayScaleAndSqueeze(object):\n    def __call__(self, img):\n        return F_tr.to_grayscale(img, num_output_channels=1)\n\ntransform_image = v2.Compose([\n    v2.Resize((image_size)),\n    v2.ToTensor(),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef to_one_hot(mask, num_classes):\n    mask = mask.long()  # Convert to LongTensor\n    one_hot_mask = torch.nn.functional.one_hot(mask, num_classes=num_classes)\n    return one_hot_mask.permute(0, 3, 1, 2)\n\ntransform_mask = v2.Compose([\n    v2.Resize((image_size)),\n    ToGrayScaleAndSqueeze(),\n    v2.ToTensor(),\n    v2.Lambda(lambda mask: (mask > 0).float()),  # Binarize the mask\n])","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.541661Z","iopub.execute_input":"2024-05-31T09:03:00.541930Z","iopub.status.idle":"2024-05-31T09:03:00.573861Z","shell.execute_reply.started":"2024-05-31T09:03:00.541906Z","shell.execute_reply":"2024-05-31T09:03:00.572896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n# Set the directories for your image and mask files\ntrain_img_dir = '/kaggle/input/easypotraitlipsonly/EasyPotraitLipsOnly/data/train/images/img'\ntrain_mask_dir = '/kaggle/input/easypotraitlipsonly/EasyPotraitLipsOnly/data/train/masks/img'\nval_img_dir = '/kaggle/input/easypotraitlipsonly/EasyPotraitLipsOnly/data/val/images/img'\nval_mask_dir = '/kaggle/input/easypotraitlipsonly/EasyPotraitLipsOnly/data/val/masks/img'\ntest_img_dir = '/kaggle/input/easypotraitlipsonly/EasyPotraitLipsOnly/data/test/images/img'\ntest_mask_dir = '/kaggle/input/easypotraitlipsonly/EasyPotraitLipsOnly/data/test/masks/img'\n\n# Check if the directories exist and are not empty\nfor dir in [train_img_dir, train_mask_dir, val_img_dir, val_mask_dir, test_img_dir, test_mask_dir]:\n    if not os.path.exists(dir):\n        print(f\"Directory does not exist: {dir}\")\n    elif not os.listdir(dir):\n        print(f\"Directory is empty: {dir}\")\n\n# Create the datasets\ntrain_dataset = SegmentationDataset(train_img_dir, train_mask_dir, transform_image=transform_image, transform_mask=transform_mask)\nval_dataset = SegmentationDataset(val_img_dir, val_mask_dir, transform_image=transform_image, transform_mask=transform_mask)\ntest_dataset = SegmentationDataset(test_img_dir, test_mask_dir, transform_image=transform_image, transform_mask=transform_mask)\n\n\nfrom torch.utils.data import DataLoader\n\nbatch_size = 64\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.575289Z","iopub.execute_input":"2024-05-31T09:03:00.575970Z","iopub.status.idle":"2024-05-31T09:03:00.643220Z","shell.execute_reply.started":"2024-05-31T09:03:00.575915Z","shell.execute_reply":"2024-05-31T09:03:00.642462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement.\n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.644313Z","iopub.execute_input":"2024-05-31T09:03:00.644615Z","iopub.status.idle":"2024-05-31T09:03:00.656157Z","shell.execute_reply.started":"2024-05-31T09:03:00.644589Z","shell.execute_reply":"2024-05-31T09:03:00.655243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_iou(output, target, smooth=1):\n    output_flat = output.view(-1)\n    target_flat = target.view(-1)\n    intersection = (output_flat * target_flat).sum()\n    union = output_flat.sum() + target_flat.sum() - intersection\n    \n    iou = (intersection + smooth) / (union + smooth)\n    return iou.mean()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.657213Z","iopub.execute_input":"2024-05-31T09:03:00.657535Z","iopub.status.idle":"2024-05-31T09:03:00.673857Z","shell.execute_reply.started":"2024-05-31T09:03:00.657509Z","shell.execute_reply":"2024-05-31T09:03:00.672940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DiceLoss(nn.Module):\n    def __init__(self, smooth=1):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, output, target):\n        output_flat = output.view(-1)\n        target_flat = target.view(-1)\n        intersection = (output_flat * target_flat).sum()\n        \n        return 1 - ((2. * intersection + self.smooth) /\n                    (output_flat.sum() + target_flat.sum() + self.smooth))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:00.675020Z","iopub.execute_input":"2024-05-31T09:03:00.675322Z","iopub.status.idle":"2024-05-31T09:03:00.688499Z","shell.execute_reply.started":"2024-05-31T09:03:00.675297Z","shell.execute_reply":"2024-05-31T09:03:00.687624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torch.optim.lr_scheduler import StepLR\n\n# Define the optimizer\noptimizer = Adam(model.parameters(), lr=0.005)\n\n# Define the learning rate scheduler\nscheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n\n# Define the number of training epochs and patience for early stopping\nnum_epochs = 80\npatience = 10\n\n# Initialize variables for early stopping\nbest_val_loss = float('inf')\nbest_model = None\nepochs_no_improve = 0\n\ncriterion = DiceLoss()\n\nprint(\"Mấy giờ rồi?\")\nprint(\"Đến giờ train model rồi!\")\n# Training loop\n\nfor epoch in range(num_epochs):\n    print(f'Epoch: {epoch+1}')\n    # Training phase\n    model.train()\n    for images, masks in train_loader:\n        images = images.to(device)\n        masks = masks.to(device)\n        masks = masks.long()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        miou = mean_iou(outputs, masks)\n        print(f'Train loss: {loss}, IoU: {miou}')\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Update the learning rate\n    scheduler.step()\n\n    # Validation phase\n    model.eval()\n    val_losses = []\n    iou_values = []\n    with torch.no_grad():\n        i = 0\n        for i, (images, masks) in enumerate(val_loader):\n            if (i==3):\n                break\n            i += 1\n            images = images.to(device)\n            masks = masks.to(device)\n            masks = masks.long()\n            outputs = model(images)\n\n            iou = mean_iou(outputs, masks)\n            loss = criterion(outputs, masks)\n            \n            val_losses.append(loss.item())\n            iou_values.append(iou)\n            \n    val_loss = np.mean(val_losses)\n    iou_mean = np.mean([iou.cpu().numpy() for iou in iou_values])\n\n    # Check for early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = deepcopy(model)\n        torch.save(best_model.state_dict(),'best_lips_segmentation_linknet.pt')\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        if epochs_no_improve == patience:\n            print('Early stopping!')\n            model = best_model\n            torch.save(model.state_dict(),'early_lips_segmentation_linknet.pt')\n            break\n    torch.save(model.state_dict(),'latest_lips_segmentation_linknet.pt')\n    \n    \n    # Print losses\n    print(f'Epoch {epoch+1}/{num_epochs}: Mean iou: {iou_mean}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#     # Test phase\n#     import torch\n#     import matplotlib.pyplot as plt\n\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n#     # Load the trained model\n#     eval_model = LinkNet(num_classes)\n#     model_path = '/kaggle/input/linknet_lip_seg/pytorch/best/1/best_lips_segmentation_linknet.pt'\n#     state_dict = torch.load(model_path)\n#     new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n#     eval_model.load_state_dict(new_state_dict)\n\n#     # # Assuming that we are on a machine with multiple GPUs.\n#     # if torch.cuda.device_count() > 1:\n#     #     print(torch.cuda.device_count(), \"GPUs!\")\n#     #     # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n#     #     eval_model = nn.DataParallel(model)\n\n#     eval_model.to(device)\n\n#     eval_model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:23.951070Z","iopub.status.idle":"2024-05-31T09:03:23.951623Z","shell.execute_reply.started":"2024-05-31T09:03:23.951340Z","shell.execute_reply":"2024-05-31T09:03:23.951363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # No gradient calculation\n# iou_test = []\n# with torch.no_grad():\n#     i = 0\n#     for images, masks in test_loader:\n#         # Move the images and masks to the device\n#         images = images.to(device)\n#         masks = masks.to(device)\n\n#         # Predict the masks\n#         outputs = eval_model(images)\n#         iou = mean_iou(outputs, masks)\n#         i += 1\n#         if i < 4:\n#             # Plot original image, true mask, and predicted mask\n#             fig, axs = plt.subplots(1, 3, figsize=(20, 20))\n#             axs[0].imshow(images[0].permute(1, 2, 0).cpu())  # Assuming image is in (C, H, W) format\n#             axs[0].title.set_text('Original Image')\n#             axs[1].imshow(masks[0].cpu(), cmap='gray')\n#             axs[1].title.set_text('True Mask')\n#             axs[2].imshow(outputs[0].squeeze().cpu(), cmap='gray')\n#             axs[2].title.set_text('Predicted Mask')\n#             plt.show()\n#         iou_test.append(iou)\n# iou_mean = np.mean([iou.cpu().numpy() for iou in iou_test])\n# print(f'Test iou: {iou_mean}')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:23.952919Z","iopub.status.idle":"2024-05-31T09:03:23.953360Z","shell.execute_reply.started":"2024-05-31T09:03:23.953128Z","shell.execute_reply":"2024-05-31T09:03:23.953147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import requests\n# from PIL import Image\n# import matplotlib.pyplot as plt\n# import torch\n# from torchvision import transforms\n\n# # Load the image\n# url = \"https://scontent.fhan7-1.fna.fbcdn.net/v/t1.6435-9/129730825_2843581455923669_4319122742225070187_n.jpg?stp=dst-jpg_p640x640&_nc_cat=111&ccb=1-7&_nc_sid=5f2048&_nc_ohc=MOcdN7aZCC4Q7kNvgEl0ux5&_nc_ht=scontent.fhan7-1.fna&oh=00_AYCqBbKCLJgME-1umNSS58LubHThzcld2EMZrLkFQdvZcQ&oe=6681136C\"\n# response = requests.get(url, stream=True)\n# img = Image.open(response.raw)\n\n# # Convert the image to a tensor and add a batch dimension\n# img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n\n# # Predict the mask\n# output = eval_model(img_tensor)\n# predicted_mask = output[0].squeeze().cpu().detach().numpy()\n\n# # Plot the original image\n# plt.imshow(img)\n\n# # Plot the predicted mask as an outer layer\n# plt.imshow(predicted_mask, alpha=0.5, cmap='jet')\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:03:23.955111Z","iopub.status.idle":"2024-05-31T09:03:23.955451Z","shell.execute_reply.started":"2024-05-31T09:03:23.955274Z","shell.execute_reply":"2024-05-31T09:03:23.955287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}